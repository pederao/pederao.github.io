<!DOCTYPE html>
<!--
	Prologue by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Peder Olsen</title>
		<meta charset="utf-8" />
		<meta
			name="viewport"
			content="width=device-width, initial-scale=1, user-scalable=no"
		/>
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">
		<!-- Header -->
		<div id="header">
			<div class="top">
				<!-- Logo -->
				<div id="logo">
					<span class="image avatar48"
						><img src="images/avatar.jpg" alt=""
					/></span>
					<h1 id="title">Peder Olsen</h1>
					<p>Mathematician and Space Farmer</p>
				</div>

				<!-- Nav -->
				<nav id="nav">
					<ul>
						<li>
							<a href="#spaceeye" id="spaceeye-link"
								><span class="icon fa-regular fa-eye">SpaceEye</span></a
							>
						</li>
						<li>
							<a href="#lasr" id="lasr-link"
								><span class="icon solid fa-bolt"> Super Resolution</span></a
							>
						</li>
						<li>
							<a href="#farming" id="farming-link"
								><span class="icon solid fa-seedling">Precision Farming</span></a
							>
						</li>
						<li>
							<a href="#contact" id="boxproduct-link"
								><span class="icon solid fa-th">Matrix Derivatives</span></a
							>
						</li>						
						<li>
							<a href="#contact" id="counting-link"
								><span class="icon solid fa-leaf">Phenotyping</span></a
							>
						</li>
						<li>
							<a href="#contact" id="insar-link"
								><span class="icon solid fa-satellite-dish">InSAR</span></a
							>
						</li>
						<li>
							<a href="#separation" id="separation-link"
								><span class="icon solid fa-microphone">Speech Separation</span></a
							>
						</li>
					</ul>
				</nav>
			</div>

			<div class="bottom">
				<!-- Social Icons -->
				<ul class="icons">
					<li>
						<a
							href="https://math.stackexchange.com/users/59704/peder"
							class="icon brands fa-stack-exchange"
							><span class="label">Stack Exchange</span></a
						>
					</li>
					<li>
						<a href="#" class="icon solid fa-list-alt"
							><span class="label">Facebook</span></a
						>
					</li>
					<li>
						<a href="https://github.com/pederao" class="icon brands fa-github"
							><span class="label">Github</span></a
						>
					</li>
					<li>
						<a href="#" class="icon brands fa-dribbble"
							><span class="label">Dribbble</span></a
						>
					</li>
					<li>
						<a href="#" class="icon solid fa-envelope"
							><span class="label">Email</span></a
						>
					</li>
				</ul>
			</div>
		</div>

		<!-- Main -->
		<div id="main">
			<!-- Intro -->
			<section id="spaceeye" class="three">
				<div class="container">
					<header>
						<h2 class="alt">
							<strong>Project SpaceEye</strong>
						</h2>
						<p>
							SpaceEye was a project to build a cloud removal system for satellite images 
							using synthetic aperture radar to "see through clouds".  The system is available
							as an API through 
							<a href="https://microsoft.github.io/farmvibes-ai/docfiles/markdown/workflow_yaml/data_ingestion/spaceeye/spaceeye.html">
							FarmVibes</a>.  The idea was to use neural attention to interpolate cloud-free patches to replace cloudy optical patches.
							This closely resembles approaches that have used temporal interpolation to fill in missing data, but here we use radar 
							data to guide the interpolation.  The system was trained on 
							<a href="https://planetarycomputer.microsoft.com/dataset/group/sentinel-1"> Sentinel-1</a> and 
							<a href="https://planetarycomputer.microsoft.com/dataset/sentinel-2-l2a"> Sentinel-2</a> data
							from the European Space Agency (via 
							<a href="https://planetarycomputer.microsoft.com/">Microsoft Planetary Computer</a>).  
							The project was based on 
							the summer internsip work of <a href="https://www.cis.upenn.edu/~mingminz/"> Mingmin Zhao</a>.  
							The published paper is available here: <a href="https://ieeexplore.ieee.org/document/10025762"> Seeing through clouds in satellite images.</a>
						</p>
						<p>Here are some example applications of SpaceEye:</p>
						<figure> 
							<img src="images/quincy_indices.jpg" width="600" alt="NDVI"/>
							<figcaption>
								Calculating vegetation indices (NDVI and NDWI) from cloudy images
							</figcaption>
						</figure>
						<figure> 
							<img src="images/luther_derecho.jpg" width="600" alt="change"/>
							<figcaption> 
								Change detection
							</figcaption>
						</figure>
						<figure> 
							<img src="images/bridgeport_nbr.jpg" width="300" alt="NBR"/>
							<figcaption> 
								Wildfire damage assessment
							</figcaption>
						</figure>
						<p>
							A video demonstration of the system can be found 
							<a href="https://youtu.be/FBCUPqFozfA"> here</a>.  In an earlier research project at IBM we 
							implemented a sophisticated nuclearn norm based interpolation version for cloud removal in daily 
							<a href="https://terra.nasa.gov/"> MODIS</a> images.  That 
							system has the potential to be as accurate, but is slower and the implementation 
							used a very poor quality cloud detector.  The following video shows a year worth of MODIS images for 
							the entire contiguous United States (the black region is not covered by TERRA MODIS). 
						</p>
						
						<figure> 
							<video height="400" src="movies/mixed.mp4" controls></video>
							<figcaption> 
								Daily cloudfree MODIS images for the contiguous United States.  The left panel shows the original
								MODIS images, the right panel shows the MODIS images with clouds removed,
							</figcaption>
						</figure>
						
					</header>

					<!-- <footer>
						<a href="#portfolio" class="button scrolly">Projects</a>
					</footer> -->
				</div>
			</section>

			<section id="lasr" class="two">
				<div class="container">
					<header>
						<h2 class="alt">
							<strong>Location Aware Super Resolution</strong>
						</h2>
						<p>
							Using satellite images for precision farming is hampered by two issues.  
							<ol type="i">
								<li>The presence of clouds and other atmospheric noise sources.</li>
								<li>The low spatial resolution of freely available satellite images.</li>
							</ol>
							With Project SpaceEye we attempted to address the first issue and in this project
							we used historical high resolution imagery to increase the resolution at a particular location.
							Part of the idea was that super-resolution with side information (location) is easier than
							blind super-resolution.  The project was done in collaboration with Olaoluwa Adigun and Ranveer 
							Chandra at Microsoft Research.  The published paper is available from 
							<a href="https://ieeexplore.ieee.org/document/9884391"> the IGARSS 2022 proceedings </a>.
							The paper used Sentinel-2 as a free low-resolution image source and Pleiades as a 
							high-resolution image source.  Special thanks to Airbus for providing the Pleiades data.
						</p>

						<figure> <img src="images/lasr_collage.jpg" alt=""/>
						<figcaption> Example of Location Aware Super Resolution (LASR) using Sentinel-2 and Pleiades images.  
							Column 1: unseen ground truth, 2: Sentinel-2, 3-6: super-resolution methods 7-10: super-resolution 
							with high resolution historical side information.  The LASR methods (7-10) are from our paper.
						</figcaption>
						</figure>

						<figure> 
							<video src="movies/lasr_zoom.mp4" controls></video>
							<figcaption> 
								A video demonstrating the performance of LASR.  The second row represents traditional super-resolution
								methods while the third row are LASR methods from our paper.  Take note of the improved image fidelity
								at the >
						</figure>highest zoom levels and the color accuracy at the lowest zoom levels.
							</figcaption


					</header>
				</div>
			</section>


			<section id="farming" class="three">
				<div class="container">
					<header>
						<h2 class="alt">
							<strong>Learning to see more: Super-resolution for precision farming.</strong>
						</h2>
					</header>
					<p>
						Unfortunately, even the best super-resolution methods cannot increase the resolution 
						reliably beyond a factor of 10.  For those precision farming applications that require
						sub-meter resolution we therefore need a new approach.  Such an approach could be the traditional
						one of flying drones over the fields, but that is expensive and not scalable.  In this project 
						we considered firstly, the possibilty of extendinging the spectral resolution of the drone images
						by fusing them with freely available multispectral satellite images.  Secondly, we considered 
						temporal and spatial extension of the drone images by use of traditional super-resolution methods.
					</p>

					<figure> <img src="images/System_Diagram_Inference_lr.png" width=600 alt=""/>
					<figcaption> Using targeted drone images we increase the spectral, spatial and temporal resolution 
						by fusing with freely available satellite images.
					</figcaption>
					</figure>
					<h3> Simulating cameras and satellite sensors</h3>
					<p>
					In order to train these systems we need high resolution images, but satellite sensors are sophisticated, 
					expensive and existing datasets are at a low-resolution.  To get around this problem we simulate the 
					sensors by using high resolution drone images and the published spectral response functions of the 
					satellite sensors.  A farmer will be unable to afford a hyperspectral camera, but once spectral 
					extension models are built he won't need one.  We used hyperspectral images collected by 
					<a href="https://www.ars.usda.gov/"> USDA-ARS</a>. 
					</p>
					<figure> <img src="images/SpectralResponseSentinel2.png" width=600 alt=""/>
					<figcaption> The normalized spectral response function published for Sentinel-2A and Sentinel-2B.
					</figcaption>
					</figure>

					<h3> Image Registration</h3>
					<p>
					To fuse the drone and satellite images we need to take into account that Sentinel-2 images are at 
					a 10m resolution and drone images are at a 2-15cm resolution.  This means that a single Sentinel-2 Pixel
					corresponds to 80x80 drone pixels for a 12.5cm drone resolution.  To register the images we took a 
					two-stage approach consisting of using the geospatial projection information to get close and then using 
					regression to get the final alignment.  
					</p>
					<figure> <img src="images/Pixel_alignment_and_image_registration.png" width=600 alt=""/>
					<figcaption> Our image registration approach.  Note the slight subtlety of having to align the upper-left
						corner of the Sentinel-2 pixel with the corresponding upper-left corner of a drone pixel.  We estimated 
						our final registration error to be approximately 0.81m (6.5 drone pixels or 0.08 Sentinel-2 pixels).
					</figcaption>
					</figure>	
					<h3> Example Results</h3>
					<p>
						For spatial extension we target 1m resolution, and the resulting reconstructions are not perfect,
						however, with spectral extension that has access to a high resolution drone images as side information 
						the results are even good enough to use as ground truth for training spatial-extension 
						super-resolution models. 
					</p>
					<figure> <img src="images/spatial_extension.png" width=600 alt=""/>
					<figcaption> Spatial extension used for a cover crop application.  The second column shows the inference results.
					</figcaption>
					</figure>									
					<figure> <img src="images/spectral_extension.png" width=600 alt=""/>
					<figcaption> Spectral extension used for a cover crop application.  The third column shows the inference 
						results.  These results are scale invariant and works equally well for drone images at 2cm or 15cm 
						resolution, but are specific to the drone and satellite sensors used.
					</figcaption>
					</figure>
					<figure> <img src="images/weed_exp.jpg" width=600 alt=""/>
					<figcaption> Spectral extension used for to detect Italian ryegrass in wheat.  This experiment was done
						with an early version of the spectral extension model that is available through FarmVibes. 
					</figcaption>
					</figure>
				</div>
			</section>


			<!-- Speech Separation -->
			<section id="separation" class="three">
				<div class="container">
					<header>
						<h2>Single Microphone Speech Separation</h2>
					</header>

					<p>
						This project was done at IBM research with Steven Rennie, John Hershey and Trausti Kristjanson 
						in the era before neural networks ruled speech recognition.  
						At the core the idea was to simply use a standard speech recognition system (HMM/GMM) to 
						search through the cartesian product of each persons word-lattice.  For best performance we
						used acoustic models specific to each speaker when feasible and a fast approximate search 
						through the product graph.  For two speakers we used the Gharamani/Jordan factorial HMM 
						algorithm and for more than two speakers loopy belief propagation.  The first example is from 
						the ICSLP 2006 Speech Separation Challenge that used speech from a small grammar artificially 
						mixed together.
					</p>

					<div class="row">
						<div class="col-4 col-12-mobile">
							<article class="item">
								
								<p class="image fit" >
									<img src="images/spec_fm.png" alt="Mixed Audio Signal"/>
								</p>
								<header>
									<h3>
										Female + Male audio (0dB mix):
									</h3>
									<button onclick="document.getElementById('audiomf').play()">
										<b style="color:DarkBlue;"> Mixed Audio </b>
									</button>
									<audio id="audiomf" src="audio/mf_mix.mp3"></audio>
								</header>
							</article>
						</div>
						<div class="col-4 col-12-mobile">
							<article class="item">
								<p class="image fit">
									<img src="images/spec_f.png" alt="Female Speaker"/>
								</p>
								<header>
									<h3>
										Recognition + reconstruction for female speaker:
									</h3>
									<button onclick="document.getElementById('audiof').play();">
										<b style="color:DarkRed;"> Bin green with F 7 now</b>
									</button>
									<audio id="audiof" src="audio/mf_mix_to_f.mp3"></audio>
								</header>
							</article>
						</div>
						<div class="col-4 col-12-mobile">
							<article class="item">
								<p class="image fit">
									<img src="images/spec_m.png" alt="Male Speaker"/>
								</p>
								<header>
									<h3>
										Recognition + reconstruction for male speaker:
									</h3>
									<button onclick="document.getElementById('audiom').play();">
										<b style="color:DarkGreen;"> Lay white at X 8 soon</b>
									</button>
									<audio id="audiom" src="audio/mf_mix_to_m.mp3"></audio>
								</header>
							</article>
						</div>
					</div>

				<p> In the second example we attempted a full-fledged recognition of a real recording from the 
					2008 primary presidential debate between then Senator Hillary Clinton and then Senator Barak Obama.  
					This audio is full of noise (reverb, word-restarts, third speakers, etc/) and consists of natural 
					speech.  It's therefore much harder and the reconstruction quality is not as good. </p>

				<div class="row">
					<div class="col-4 col-12-mobile">
						<article class="item">
							<header>
								<button onclick="document.getElementById('audiocnn').play()">
									<b style="color:DarkBlue;"> CNN Debate 2008 </b>
								</button>
								<audio id="audiocnn" src="audio/cnn.mp3"></audio>
							</header>
						</article>
					</div>
					<div class="col-4 col-12-mobile">
						<article class="item">
							<header>
								<button onclick="document.getElementById('audioclinton').play();">
									<b style="color:DarkRed;"> Then Senator Hillary Clinton</b>
								</button>
								<audio id="audioclinton" src="audio/clinton.mp3"></audio>
							</header>
						</article>
					</div>
					<div class="col-4 col-12-mobile">
						<article class="item">
							<header>
								<button onclick="document.getElementById('audiom').play();">
									<b style="color:DarkGreen;"> Then Senator Barack Obama</b>
								</button>
								<audio id="audiom" src="audio/obama.mp3"></audio>
							</header>
						</article>
					</div>
				</div>

				<p> With the more constrained grammar we could even handle four speakers: </p>					

				<div class="row">
					<div class="col-4 col-12-mobile">
						<article class="item">
							<header>
								<button onclick="document.getElementById('audiomix4').play()">
										<b style="color:DarkBlue;"> Mix of four speakers </b>
								</button>
							<audio id="audiomix4" src="audio/mix4_all.mp3"></audio>
							</header>
							</article>
						</div>
						<div class="col-4 col-12-mobile">
							<article class="item">
								<header>
									<button onclick="document.getElementById('mix41').play();">
										<b style="color:DarkRed;"> Place white at D 0 soon</b>
									</button>
									<audio id="mix41" src="audio/mix4_1.mp3"></audio>
								</header>
							</article>
						</div>
						<div class="col-4 col-12-mobile">
							<article class="item">
								<header>
									<button onclick="document.getElementById('mix42').play();">
										<b style="color:DarkGreen;"> Place red in H 3 now</b>
									</button>
									<audio id="mix42" src="audio/mix4_2.mp3"></audio>
								</header>
							</article>
						</div>
					</div>

					<div class="row">
					<div class="col-4 col-12-mobile">
						<article class="item">
							<header>
								<h3> Speakers 3 and 4:</h3>
							</header>
							</article>
						</div>
						<div class="col-4 col-12-mobile">
							<article class="item">
								<header>
									<button onclick="document.getElementById('mix43').play();">
										<b style="color:Purple;"> Lay blue at P 0 now</b>
									</button>
									<audio id="mix43" src="audio/mix4_3.mp3"></audio>
								</header>
							</article>
						</div>
						<div class="col-4 col-12-mobile">
							<article class="item">
								<header>
									<button onclick="document.getElementById('mix44').play();">
										<b style="color:Maroon;"> Place green with B 8 soon</b>
									</button>
									<audio id="mix44" src="audio/mix4_4.mp3"></audio>
								</header>
							</article>
						</div>
					</div>


				</div>
			</section>



		</div>

		<!-- Footer -->
		<div id="footer">
			<!-- Copyright -->
			<ul class="copyright">
				<li>&copy; Untitled. All rights reserved.</li>
				<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
			</ul>
		</div>

		<!-- Scripts -->
		<script src="assets/js/jquery.min.js"></script>
		<script src="assets/js/jquery.scrolly.min.js"></script>
		<script src="assets/js/jquery.scrollex.min.js"></script>
		<script src="assets/js/browser.min.js"></script>
		<script src="assets/js/breakpoints.min.js"></script>
		<script src="assets/js/util.js"></script>
		<script src="assets/js/main.js"></script>
		<!-- JavaScript -- add at the bottom of your body tag -->
		<script> 
		function play(){
			var audio = document.getElementById('audio')
			audio.play();
		}
		</script>
	</body>
</html>
